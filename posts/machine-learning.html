
<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content=". Kabir Shah's blog.">
  <meta name="author" content="Kabir Shah">

  <title>Machine Learning | Kabir Shah</title>
  <link href="https://fonts.googleapis.com/css?family=Inconsolata|Merriweather:300,300i" rel="stylesheet">

  <link rel="stylesheet" type="text/css" href="../fonts/fonts.css" />
  <link rel="stylesheet" type="text/css" href="../css/lib/wing.min.css" />
  <link rel="stylesheet" type="text/css" href="../css/post.css" />

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-70792533-7', 'auto');
    ga('send', 'pageview');
  </script>
</head>

<body>
  <a href="../" id="back"><img src="../img/back.svg" alt="Go Back" /></a>
  <div class="container">
    <h1 class="post-title">Machine Learning</h1>
    <h3 class="post-date">April 29, 2017</h3>
    <div class="post-content">
      <p>Artificial Intelligence is a topic that has been attracting a lot of interest from people lately, myself included. Around 6 months ago, I became interested in machine learning, specifically neural networks. There are a couple great lectures from Stanford, and some nice articles out there that explain the topic nicely.</p>
<p>Nonetheless, most of them included complex mathematic notations and often used a machine learning framework to show code samples. While the frameworks are great, they I think it is just as important to know what exactly goes on under the hood.</p>
<p>This will be a series of blog posts set to help you understand how machine learning works, and have the code to go with it. The code samples will use Python, with NumPy. NumPy allows for complex math operations to be extremely simple in code, and is not specifically for machine learning. We will be using it for matrix multiplication, dot products, etc.</p>
<h2 id="what-is-machine-learning-">What is Machine Learning?</h2>
<p>Machine learning essentially allows for a machine to learn patterns between certain items without any of it having to be hard-coded or specified. The machine itself learns these patterns as it trains.</p>
<p>The flexibility of machine learning allows it to be used for a variety of topics. They are used to solve problems such as <strong>classification</strong> and <strong>regression</strong> problems.</p>
<p>I&#39;ll use an example of emails and spam to explain these two types of problems.</p>
<p>Classification problems are problems in which an input is taken in, and it is classified into a certain group.</p>
<p><img src="../img/machine-learning/classification.svg" alt="Classification Visual"></p>
<p>Regression problems are problems in which an input is taken in, and there is an output that doesn&#39;t correspond to a group.</p>
<p><img src="../img/machine-learning/regression.svg" alt="Regression Visual"></p>
<p>There are also <strong>two types of learning</strong> methods: supervised and unsupervised.</p>
<p>Supervised learning is when you have a set of inputs and outputs, and you train the machine on that. Unsupervised learning is when you only have a set of inputs, and you train the machine to find patterns between them.</p>
<h2 id="feedforward-neural-networks">Feedforward Neural Networks</h2>
<p>One method of machine learning is to use a <strong>Feedforward Neural Network</strong>. They work by:</p>
<ul>
<li>Taking an input (a 2D array of numbers)</li>
<li>Multiplying the input by a certain set of weights</li>
<li>Applying an <strong>activation function</strong></li>
<li>Returning an output</li>
</ul>
<p><img src="../img/machine-learning/FeedForwardNeuralNetwork.svg" alt="Feedforward Neural Network"></p>
<p>Those <strong>weights</strong> are where the magic happens. The neural network has to find the perfect set of weights to get the desired output, after starting with a random set of weights. The act of multiplying the inputs by the weights to form an output is <strong>forward propagation</strong>, as you are moving the inputs through the network. The activation function is just a function that can squash a value between a certain range, it introduces <strong>nonlinearity</strong> into the model.</p>
<p>The multiplication of the inputs to weights to convert it into the shape of the output is a <strong>dot product</strong>.</p>
<p>In terms of math, we are doing the following:</p>
<script type="math/tex">activation((X W_h) + b_h)</script>

<p>Where <script type="math/tex">X</script> is the input, <script type="math/tex">W_h</script> is the set of weights, and <script type="math/tex">b_h</script> is the bias.</p>
<h4 id="back-propagation">Back Propagation</h4>
<p>We have a random set of weights initially, and we need to get them to the perfect place, where all of our inputs passed through the network are equal to their corresponding outputs.</p>
<p>First, we need a way to see how far our network was. We can do that by using a <strong>loss function</strong>. We&#39;ll use the <strong>mean sum squared</strong> loss function, represented mathematically as:</p>
<script type="math/tex">l(o, y) = \sum 0.5(o - y)^2</script>

<p>Where <code>o</code> is the output of our network, and <code>y</code> is the target output.</p>
<p>Now that he have a loss function, we need to find a way to get it to equal 0.</p>
<p>You might think that the best way doing this is to try all of the possible weights until we get a good result. While that might work for extremely small networks, when you get hundreds of thousands of weights, it may take <em>years</em> to compute.</p>
<p>Instead of that, what if we could track exactly what the weights should change by to decrease the loss? That is what a <strong>derivative</strong> is for.</p>
<p>We can find the derivative of the loss function with respect to the weights. This allows us adjust the weights in the correct way in order to lower the loss.</p>
<p>Let&#39;s visualize this by graphing a range of weights and their corresponding loss.</p>
<p><img src="../img/machine-learning/weightToLoss.svg" alt="Weight to Loss Visual"></p>
<p>If we find the derivative of the loss function with respect to the weights, we can find our way downhill from where we are, and move a little closer to our goal: having a loss of 0.</p>
<p>First, let&#39;s go through an example of how a derivative works.</p>
<p>For simplicity, let&#39;s have a simple function that takes some input <code>X</code> and returns it multiplied by a weight <code>w</code>.</p>
<script type="math/tex">f(X, w) = X w</script>

<p>The derivative of this function with respect to the weight is:</p>
<script type="math/tex">\frac{\partial f}{\partial w} = X</script>

<p>We need to find the effect the weight has on <code>X</code>. Let&#39;s use a weight of <code>5</code>, and an input of <code>2</code>. If we plug it into the derivative function, we get <code>2</code> as a result.</p>
<p>That means that if we change the weights by one, then the output of the function will increase by <code>2</code>, and it does!</p>
<p>Now, we have to do the same thing, but for our loss function, with respect to our weights. This gives us a <strong>gradient</strong> of how much our loss will <em>increase</em> based on how we change our weights. A gradient is basically the derivative of all of the inputs in a vector. All we have to do after that, is <em>decrease</em> our weights by the gradient, and we will decrease the loss!</p>
<p>Let&#39;s find the partial derivative of the loss function with respect to some weights.</p>
<script type="math/tex">\frac{\partial l}{\partial w}</script>

<p>If we use the chain rule, we get:</p>
<script type="math/tex">\frac{\partial l}{\partial w} = \frac{\partial l}{\partial o} * \frac{\partial o}{\partial h} * \frac{\partial h}{\partial w}</script>

<p>Let&#39;s find all parts of the equation:</p>
<script type="math/tex">\frac{\partial l}{\partial o} = o - y\\
\frac{\partial o}{\partial h} = \frac{e^{-o}}{\left(1\ +e^{-o}\right)^2}\\
\frac{\partial h}{\partial w} = X^\intercal
</script>

<p>We can multiply all of them, and we&#39;ll have the gradients! Now we&#39;ll know exactly what will happen as a result of updating our weights in a certain direction, and can push them into the direction that makes the loss function zero.</p>
<h2 id="the-problem-coming-soon-">The Problem (Coming Soon)</h2>
<h2 id="the-code-coming-soon-">The Code (Coming Soon)</h2>
<h2 id="conclusion">Conclusion</h2>
<p>This article is a work in progress. Feel free to give any suggestions or fixes.
<!-- Now that we know how a neural network works, we can begin coding this up in Python/NumPy. --></p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    </div>
  </div>
  <div id="disqus_thread"></div>
  <script type="text/javascript" src="../js/comments.js"></script>
  <script type="text/javascript" src="../js/scripts.js"></script>
</body>

</html>
